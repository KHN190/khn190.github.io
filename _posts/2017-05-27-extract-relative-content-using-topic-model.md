---
layout: post
title:  "使用 Latent Dirichlet Allocation 主题模型"
cover: null.png
date:   2017-05-27 16:00:00 +0800
categories: machine-learning
---

不久以前，有一个需要是从百度知道的几百万条相似问题对中，抽出和我们的系统会话相关的问题……

最容易想到的是关键字抽取：统计会话中的高频关键词，用正则把百度问题抽出来。关键词可以简单通过分词 + 统计词频 + 排序得到。但实际实施发现简单分词很难达到理想效果，因为这是个客服系统，会话通常是多轮，导致出现大量需要语义才能判断是否为有意义的词。

比如：“退回”，“已付”，“审查”等等，在一些情况下它是一个句子的核心词，但另一些情况下它只是一个普通出现的词。通过词频无法判断其是否有意义。怎么能识别哪些词才是真正在所有会话中普遍有意义的？

故事要从概率论讲起……

## 1. LDA 模型的简明阐释

Latent Dirichlet Allocation 在 2003 年提出，它假设一篇文章有(k)个主题，以某种概率分布在(n)个词中，也就是每个主题下有多个核心词，而一篇文档有多个主题。

这种概率是在 Dirichlet 过程的迭代中得到的，即通过观察词语 w 来更新潜在的主题分布 Z，[蒙特卡洛方法](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) 和数学中传统的 [变分法](https://en.wikipedia.org/wiki/Calculus_of_variations) 都可以作为有效的求解方法。

## 2. 贝叶斯模型(Bayesian Method)

Dirichlet 过程是一个贝叶斯统计过程。先了解几个概念。

### a. 先验概率(prior)

在离散的（每次观测彼此不产生相关影响的）实验过程中，贝叶斯方法要求我们通过n次观测去更新所假定的概率，在观测前假定的概率分布称为先验概率。

事实上贝叶斯方法在认知中具有重要的地位。比如，在抛一枚硬币之前，我们最初普遍会假定一枚硬币的两面都是 0.5 的概率。但抛了几次发现概率不对，所以会怀疑这枚硬币作了弊。最初的 0.5 就是先验概率。

### b. 后验概率(posterior)

贝叶斯方法中，观察后得到的概率就是后验概率。比如我们认为硬币两面的概率不正常，所判断的基准就是对比最初的先验概率 0.5，和观察后所判断的后验概率。

### c. 共轭函数(conjugate)

在数学上，如果观察不改变先验概率的函数性质，也就是，这个函数仍然可以描述后验概率的分布，该函数称为该贝叶斯过程的共轭函数。共轭的性质方便了迭代计算。

## 3. Dirichlet 分布

讲讲二元的 Bernoulli 过程。老问题，扔硬币。一面向上，一面向下，结果只可能有两个，因此是二元。

假设扔了四次，通过观察到的结果来推断正反面 a, b 出现的概率。因为推断不可能准确（它不是一个确定的 0.5 或 0.2 等等），所以只能用一个概率的密度函数 f(p) 来表示某一面的概率区间。不断地抛硬币，我们就能不断接近最终的准确概率，这个过程叫作 Bernoulli 过程。

*tips. 数学上，这种描述函数的函数称为“[泛函](https://zh.wikipedia.org/wiki/%E6%B3%9B%E5%87%BD)”，其数学解法是变分法，蒙特卡洛方法是一种取代数值求解的物理方法。*

二元的概率分布区间可以用 B 分布(beta distribution)描述。多元的概率分布区间可以用 Dirichlet 分布描述，比如扔骰子，有六个可能的结果，就是六维的 Dirichlet 分布。

而这两个函数因为具有共轭的性质，可以方便地迭代：

> P(q\|x) P(x) = P(X=x\|q) P(q)

其中 P(q) 是观测到 x 发生之前的 q 的概率分布。P(q\|x) 是观测到 x 发生之后的 q 的概率分布。P(x) 的观测值是确定的，所以在这个确定的过程中 x 的发生是一个常量（虽然我们不知道它是多少）。所以：

> P(q\|x) ~ P(x\|q) P(q)

如果把 [B分布的密度函数](https://en.wikipedia.org/wiki/Beta_distribution) 代入进去，会发现两边形式上是等价的。左边是一个B分布，右边也是。因此证明其共轭性质。对于 Dirichlet 分布也会发现同样的事情。

## 4. LDA主题模型的应用

……回到最初的问题，怎么抽取和会话相关的百度问答问题？有了 LDA 模型，很容易想到通过主题聚类，选出所有会话中 k 个主题下排名前 30 的词语，通过已有的近义词数据库抽出相关的词语，再通过这些词语去抽取百度问答。

LDA模型是在 Spark 上跑的，例如这个程序：[LDA wikipedia example (gist)](https://gist.github.com/feynmanliang/3b6555758a27adcb527d)。这个脚本在几百万维基百科上得到100个主题，具其 [博客](https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-apache-spark.html) 称效果相当不错。

另外一篇讲义提到了先验概率对 LDA 模型的重要影响，还讲到一些训练中的实用方法：[wallach lecture](http://people.cs.umass.edu/~wallach/talks/priors.pdf) 例如，如何选取合适的主题数目，先验概率的影响，Dirichlet 分布中的两个α和β分别意味什么？强烈推荐。

我得到的结果也不错。
